{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":125981,"databundleVersionId":14910697,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n   # Imports\nimport json\nimport math\nimport heapq\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n            \n \n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T05:36:54.376880Z","iopub.execute_input":"2025-12-27T05:36:54.377386Z","iopub.status.idle":"2025-12-27T05:36:58.101562Z","shell.execute_reply.started":"2025-12-27T05:36:54.377336Z","shell.execute_reply":"2025-12-27T05:36:58.101010Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nimport csv\nimport json\nimport heapq\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\nfrom torchvision import transforms, models, datasets\n\n# PATH CONFIGURATION \nif Path(\"/kaggle/input\").exists():\n    BASE_INPUT = Path(\"/kaggle/input/the-blind-flight-synapse-drive-ps-1/SynapseDrive_Dataset\")\n    WORKING_DIR = Path(\"/kaggle/working\")\nelse:\n    BASE_INPUT = Path(\"SynapseDrive_Dataset\") \n    WORKING_DIR = Path(\".\")\n\nTRAIN_IMG_DIR = BASE_INPUT / \"train/images\"\nTRAIN_LABEL_DIR = BASE_INPUT / \"train/labels\" \nTEST_IMG_DIR = BASE_INPUT / \"test/images\"\nTEST_VEL_DIR = BASE_INPUT / \"test/velocities\"\n\nTILES_DIR = WORKING_DIR / \"train_dataset_tiles\"\nMODEL_RESNET = WORKING_DIR / \"tile_classifier_resnet.pth\"\nMODEL_EFFNET = WORKING_DIR / \"tile_classifier_effnet.pth\"\nSUBMISSION_FILE = WORKING_DIR / \"submission.csv\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 32\nEPOCHS = 8 \nLR = 0.0005\n\n#1. DATA PREPARATION \ndef step_1_prepare_data():\n    print(f\"\\n[1/4] Preparing Data (Slicing Maps from JSON)...\")\n    if TILES_DIR.exists():\n        shutil.rmtree(TILES_DIR)\n    \n    # Creating folders for classes 0, 1, 2, 3, 4\n    for i in range(5):\n        (TILES_DIR / str(i)).mkdir(parents=True, exist_ok=True)\n        \n    image_files = sorted(list(TRAIN_IMG_DIR.glob(\"*.png\")))\n    \n    tile_count = 0\n    for img_path in tqdm(image_files):\n        # Finding corresponding JSON label file\n        json_path = TRAIN_LABEL_DIR / f\"{img_path.stem}.json\"\n        \n        if not json_path.exists():\n            print(f\"Warning: No label found for {img_path.name}, skipping.\")\n            continue\n            \n        # Loading Image\n        img = Image.open(img_path).convert(\"RGB\")\n        w, h = img.size\n        \n        # Loading JSON Grid\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n            grid = np.array(data[\"grid\"]) # 20x20\n        \n        # Calculating tile size\n        grid_rows, grid_cols = grid.shape\n        tile_w, tile_h = w // grid_cols, h // grid_rows\n        \n        # Slicing and Saving\n        for r in range(grid_rows):\n            for c in range(grid_cols):\n                left, top = c * tile_w, r * tile_h\n                right, bottom = left + tile_w, top + tile_h\n                \n                # Cropping Image Tile\n                tile_img = img.crop((left, top, right, bottom))\n                \n                # Getting Label directly from the JSON grid\n                label = grid[r, c]\n                \n                # Saving\n                save_path = TILES_DIR / str(label) / f\"{img_path.stem}_{r}_{c}.png\"\n                tile_img.save(save_path)\n                tile_count += 1\n                \n    print(f\"Data Prepared: {tile_count} tiles generated.\")\n\n#2. TRAINING  \n\ndef train_model(model_name, save_path):\n    print(f\"\\n[Training] {model_name}...\")\n    \n    # Transformations\n    train_tf = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    val_tf = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    # Datasets\n    full_ds = datasets.ImageFolder(TILES_DIR, transform=train_tf)\n    train_len = int(0.8 * len(full_ds))\n    val_len = len(full_ds) - train_len\n    train_ds, val_ds = random_split(full_ds, [train_len, val_len])\n    val_ds.dataset.transform = val_tf\n    \n    # Weights\n    targets = [full_ds.targets[i] for i in train_ds.indices]\n    counts = np.bincount(targets)\n    weights = 1. / np.maximum(counts, 1)\n    sample_weights = [weights[t] for t in targets]\n    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n    \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    # Model Selection\n    if model_name == \"resnet\":\n        model = models.resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, 5)\n    else: # efficientnet\n        model = models.efficientnet_b0(pretrained=True)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 5)\n        \n    model = model.to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=LR)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n    \n    best_acc = 0.0\n    \n    for epoch in range(EPOCHS):\n        model.train()\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n                outputs = model(inputs)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        acc = correct / total\n        scheduler.step(acc)\n        print(f\"  Ep {epoch+1}: Acc {acc:.4f}\")\n        \n        if acc >= best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), save_path)\n            \n    print(f\"Saved {model_name} with Acc: {best_acc:.4f}\")\n\n# 3. SUBMISSION (Ensemble + TTA + GPS Fix)\n\ndef step_4_generate_submission():\n    print(f\"\\n[4/4] Generating Ensemble Submission...\")\n    \n    # 1. Loading Brain 1 (ResNet)\n    m1 = models.resnet18(pretrained=False)\n    m1.fc = nn.Linear(m1.fc.in_features, 5)\n    m1.load_state_dict(torch.load(MODEL_RESNET, map_location=DEVICE))\n    m1.to(DEVICE).eval()\n    \n    # 2. Loading Brain 2 (EffNet)\n    m2 = models.efficientnet_b0(pretrained=False)\n    m2.classifier[1] = nn.Linear(m2.classifier[1].in_features, 5)\n    m2.load_state_dict(torch.load(MODEL_EFFNET, map_location=DEVICE))\n    m2.to(DEVICE).eval()\n    \n    # 3. Setting up Dataset\n    class TestDS(Dataset):\n        def __init__(self, d):\n            self.files = sorted(list(d.glob(\"*.png\")))\n            self.tf = transforms.Compose([\n                transforms.Resize((224, 224)), transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n        def __len__(self): return len(self.files)\n        def __getitem__(self, idx):\n            p = self.files[idx]\n            img = Image.open(p).convert(\"RGB\")\n            # TTA: Normal + Flip\n            tiles = []\n            for im in [img, img.transpose(Image.FLIP_LEFT_RIGHT)]:\n                w, h = im.size\n                tw, th = w // 20, h // 20\n                for r in range(20):\n                    for c in range(20):\n                        tile = im.crop((c*tw, r*th, (c+1)*tw, (r+1)*th))\n                        tiles.append(self.tf(tile))\n            return p.stem, torch.stack(tiles)\n\n    ds = TestDS(TEST_IMG_DIR)\n    dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=2)\n    \n    # 4. Biome & Dijkstra Logic\n    BIOME_COSTS = {\n        \"lab\": {0:1.0, 1:9999.0, 2:3.0, 3:1.0, 4:2.0},\n        \"forest\": {0:1.5, 1:9999.0, 2:2.8, 3:1.5, 4:2.5},\n        \"desert\": {0:1.2, 1:9999.0, 2:3.7, 3:1.2, 4:2.2}\n    }\n    \n    results = []\n    \n    with torch.no_grad():\n        for fid, batch in tqdm(dl):\n            curr_id = fid[0]\n            \n            # Biome (20x20 Logic)\n            raw = Image.open(TEST_IMG_DIR / f\"{curr_id}.png\").convert(\"RGB\")\n            sm = raw.resize((20,20))\n            pix = list(sm.getdata())\n            g_votes = sum(1 for r,g,b in pix if g > r+15 and g > b+15)\n            y_score = sum((r+g)-(2*b) for r,g,b in pix)\n            biome = \"lab\"\n            if g_votes > 10: biome = \"forest\"\n            elif y_score/400 > 40: biome = \"desert\"\n            costs = BIOME_COSTS[biome]\n            \n            # Ensemble Prediction\n            inp = batch.squeeze(0).to(DEVICE) # [800, 3, 224, 224]\n            out1 = torch.softmax(m1(inp), dim=1).cpu().numpy()\n            out2 = torch.softmax(m2(inp), dim=1).cpu().numpy()\n            avg = (out1 + out2) / 2.0\n            \n            # TTA Merge(avg of both)\n            p_norm = avg[:400].reshape(20, 20, 5)\n            p_flip = avg[400:].reshape(20, 20, 5)\n            p_final = (p_norm + np.fliplr(p_flip)) / 2.0\n            \n            grid = np.argmax(p_final, axis=2)\n            \n            # GPS Fix\n            if 3 not in grid: grid[divmod(np.argmax(p_final[:,:,3]), 20)] = 3\n            if 4 not in grid:\n                 r, c = divmod(np.argmax(p_final[:,:,4]), 20)\n                 if grid[r,c] != 3: grid[r,c] = 4\n            \n            # Dijkstra\n            vel_p = TEST_VEL_DIR / f\"{curr_id}.json\"\n            v_grid = np.array(json.load(open(vel_p))[\"boost\"]) if vel_p.exists() else np.zeros((20,20))\n            \n            starts = np.argwhere(grid==3)\n            goals = np.argwhere(grid==4)\n            start = tuple(starts[0]) if len(starts)>0 else (0,0)\n            goal = tuple(goals[0]) if len(goals)>0 else (19,19)\n            \n            pq = [(0, start[0], start[1], \"\")]\n            vis = {}\n            path_str = \"r\" # default\n            \n            while pq:\n                c, r, c_idx, p = heapq.heappop(pq)\n                if (r, c_idx) == goal:\n                    path_str = p\n                    break\n                if (r, c_idx) in vis and vis[(r, c_idx)] <= c: continue\n                vis[(r, c_idx)] = c\n                \n                for dr, dc, char in [(-1,0,'u'), (1,0,'d'), (0,-1,'l'), (0,1,'r')]:\n                    nr, nc = r+dr, c_idx+dc\n                    if 0<=nr<20 and 0<=nc<20:\n                        ct = grid[nr, nc]\n                        base = costs.get(ct, 1.0)\n                        boost = v_grid[nr][nc]\n                        cost_step = max(0.01, base - boost)\n                        heapq.heappush(pq, (c + cost_step, nr, nc, p + char))\n            \n            results.append([curr_id, path_str])\n            \n    with open(SUBMISSION_FILE, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([\"image_id\", \"path\"])\n        writer.writerows(results)\n    print(f\"DONE! Saved to {SUBMISSION_FILE}\")\n\n# MAIN EXECUTION\n\nif __name__ == \"__main__\":\n    step_1_prepare_data()\n    \n    print(\"\\n[2/4] Training Brain 1 (ResNet)...\")\n    train_model(\"resnet\", MODEL_RESNET)\n    \n    print(\"\\n[3/4] Training Brain 2 (EfficientNet)...\")\n    train_model(\"efficientnet\", MODEL_EFFNET)\n    \n    step_4_generate_submission()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T05:37:20.753309Z","iopub.execute_input":"2025-12-27T05:37:20.754029Z","iopub.status.idle":"2025-12-27T11:21:24.187664Z","shell.execute_reply.started":"2025-12-27T05:37:20.754005Z","shell.execute_reply":"2025-12-27T11:21:24.186657Z"}},"outputs":[{"name":"stdout","text":"\n[1/4] Preparing Data (Slicing Maps from JSON)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:10<00:00,  1.94it/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stdout","text":"Data Prepared: 8000 tiles generated.\n\n[2/4] Training Brain 1 (ResNet)...\n\n[Training] resnet...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n","output_type":"stream"},{"name":"stdout","text":"  Ep 1: Acc 1.0000\n  Ep 2: Acc 1.0000\n  Ep 3: Acc 1.0000\n  Ep 4: Acc 1.0000\n  Ep 5: Acc 1.0000\n  Ep 6: Acc 1.0000\n  Ep 7: Acc 1.0000\n  Ep 8: Acc 1.0000\nSaved resnet with Acc: 1.0000\n\n[3/4] Training Brain 2 (EfficientNet)...\n\n[Training] efficientnet...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 184MB/s]\n","output_type":"stream"},{"name":"stdout","text":"  Ep 1: Acc 1.0000\n  Ep 2: Acc 1.0000\n  Ep 3: Acc 1.0000\n  Ep 4: Acc 1.0000\n  Ep 5: Acc 1.0000\n  Ep 6: Acc 1.0000\n  Ep 7: Acc 1.0000\n  Ep 8: Acc 1.0000\nSaved efficientnet with Acc: 1.0000\n\n[4/4] Generating Ensemble Submission...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n100%|██████████| 10000/10000 [5:36:08<00:00,  2.02s/it] ","output_type":"stream"},{"name":"stdout","text":"DONE! Saved to /kaggle/working/submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2}]}